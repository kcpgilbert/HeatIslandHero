{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63983ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Keras utils\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Dense, Normalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f518f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the text file\n",
    "county_names = []\n",
    "with open('counties.txt','r') as file:\n",
    "    # reading each line   \n",
    "    for line in file:   \n",
    "        county_names += [line.replace('\\n','')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse historic data as inputs\n",
    "for i in range(len(county_names)):\n",
    "    dataset_name = \"data/CHAT-\"+str(county_names[i])+\"-historical.csv\"\n",
    "    if i == 0:\n",
    "        pd_data_historic = pd.read_csv(dataset_name)\n",
    "    else:\n",
    "        pd_data_remain = pd.read_csv(dataset_name)\n",
    "        pd_data_historic = pd.concat([pd_data_historic,pd_data_remain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bbe3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse UHII values, duplicate y values for different X values\n",
    "for i in range(len(county_names)):\n",
    "    dataset_name = \"data/CHAT-\"+str(county_names[i])+\"-vulnerability-indicators.csv\"\n",
    "    if i == 0:\n",
    "        pd_data_uhii_base = pd.read_csv(dataset_name)\n",
    "        pd_data_uhii = pd_data_uhii_base \n",
    "        for j in range(4):\n",
    "            pd_data_uhii = pd.concat([pd_data_uhii,pd_data_uhii_base])  \n",
    "  \n",
    "    else:\n",
    "        pd_data_remain_base = pd.read_csv(dataset_name)\n",
    "        pd_data_remain = pd_data_remain_base\n",
    "        for j in range(4):\n",
    "            pd_data_remain = pd.concat([pd_data_remain,pd_data_remain_base])\n",
    "        pd_data_uhii = pd.concat([pd_data_uhii,pd_data_remain])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and Outputs, fill zeros for NaN\n",
    "X_all = pd_data_historic[['time_of_year', 'socioeconomic_group','avg_event_rh_max_perc','avg_event_rh_min_perc','tmax','tmin','hist_avg_annual_events','hist_avg_duration']]\n",
    "y = pd_data_uhii[[\"uhii_avgdeltat\"]].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43432ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for inputs and outputs, add categorical features\n",
    "X_all[\"time_of_year\"] = X_all[\"time_of_year\"].replace({\n",
    "    \"Total\": 0,\n",
    "    \"AM\": 1,\n",
    "    \"JJA\": 2,\n",
    "    \"SO\": 3})\n",
    "\n",
    "X_all[\"socioeconomic_group\"] = X_all[\"socioeconomic_group\"].replace({\n",
    "    \"2006 HW\": 0,\n",
    "    \"Vulnerable\": 1,\n",
    "    \"General\": 2\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we are having reproducable results\n",
    "import random as python_random\n",
    "np.random.seed(12345)\n",
    "python_random.seed(12345)\n",
    "tf.random.set_seed(12345)\n",
    "\n",
    "# Split the dataset\n",
    "randomseed = 12345\n",
    "indx_bin=np.arange(0,len(y))\n",
    "random.Random(randomseed).shuffle(indx_bin)\n",
    "\n",
    "train_split=0.6\n",
    "val_split=0.2\n",
    "test_split=0.2\n",
    "\n",
    "nTrain=int(train_split*len(indx_bin))\n",
    "nVal=int(val_split*len(indx_bin))\n",
    "nTest=int(test_split*len(indx_bin))\n",
    "train_indx_bin=indx_bin[0:nTrain]\n",
    "val_indx_bin=indx_bin[nTrain:nTrain+nVal]\n",
    "test_indx_bin=indx_bin[nTrain+nVal:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train=X_all.values[train_indx_bin]\n",
    "y_train=y.values[train_indx_bin]\n",
    "X_val=X_all.values[val_indx_bin]\n",
    "y_val=y.values[val_indx_bin]\n",
    "X_test=X_all.values[test_indx_bin]\n",
    "y_test=y.values[test_indx_bin]\n",
    "\n",
    "standardize=True\n",
    "\n",
    "# Apply normalization to the training dataset\n",
    "if standardize:\n",
    "    normalize_layer = Normalization()\n",
    "    normalize_layer.adapt(X_train)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ef5f7",
   "metadata": {},
   "source": [
    "## Keras ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fffd929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model parameters\n",
    "fc1 = 128\n",
    "fc2 = 64\n",
    "fc3 = 32\n",
    "fc4 = 16\n",
    "lr = 0.001\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "model.add(normalize_layer)\n",
    "model.add(Dense(fc1, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(fc2, activation='relu'))\n",
    "model.add(Dense(fc3, activation='relu'))\n",
    "model.add(Dense(fc4, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=opt, \n",
    "              loss='mae',\n",
    "              metrics=['mae'])\n",
    "              \n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=50,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbc73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "training = True\n",
    "\n",
    "if training == True:\n",
    "    history = model.fit(X_train, y_train, \n",
    "              epochs=1000,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(X_val,y_val))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.plot(history.history['mae'], label='train', color = (208/255,222/255,198/255))\n",
    "plt.plot(history.history['val_mae'], label='val', color = (78/255,105/255,86/255))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = \"model_3\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b44d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print accuracies\n",
    "pred_train= model.predict(X_train)\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "\n",
    "pred_val= model.predict(X_val)\n",
    "scores2 = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Accuracy on validation data: %1.2f (%%)'%(scores2[1]*100)) \n",
    "\n",
    "pred_test= model.predict(X_test)\n",
    "scores3 = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy on testing data: %1.2f (%%)'%(scores3[1]*100))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
